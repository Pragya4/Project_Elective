{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Elective (IIIT-B)\n",
    "### Part 1 : This notebook deals with basics of cleaning and preprocessing data .\n",
    "\n",
    "\n",
    "#### Project mentor: Prof. Manish Gupta, Onkar Hoysala "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"In case of jupyter notebook , used this if there is no pre-existing environment\"\"\"\n",
    "import sys\n",
    "!{sys.executable} -m pip install pandas\n",
    "!{sys.executable} -m pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nptel_raw_data = pd.read_csv('nptel_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nptel_raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install nltk\n",
    "# Explicitely downloaded stopwords using: python3 -m nltk.downloader stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "### Stemming\n",
    "#### With stemming, words are reduced to their word stems. A word stem need not be the same root as a dictionary-based morphological root, it just is an equal to or smaller form of the word.\n",
    "#### Can be improved further by lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import re\n",
    "porter_stemmer = nltk.stem.porter.PorterStemmer()\n",
    "\"\"\"\n",
    "This function is basically used for stemming\n",
    "\"\"\"\n",
    "def porter_tokenizer(text, stemmer=porter_stemmer):\n",
    "    tokens = nltk.wordpunct_tokenize(text)\n",
    "    stems = [porter_stemmer.stem(t) for t in tokens]\n",
    "    no_punct = [s for s in stems if re.match('^[a-zA-Z]+$', s) is not None]\n",
    "    return no_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vtt_content=nptel_raw_data.vtt_Content.tolist()\n",
    "stemmed_vtt_content=[] \n",
    "#stemming each word for each row of vtt_description of videos\n",
    "for eachcontent in vtt_content:\n",
    "    eachcontent=eachcontent.lower()\n",
    "    #In case python 2, or if need utf-8 decoding error encountered,use: eachcontent=eachcontent.lower().decode('utf-8')\n",
    "    tokenized=porter_tokenizer(eachcontent)\n",
    "    preprocessed=' '.join(tokenized)\n",
    "    preprocessed=preprocessed.encode('ascii','ignore')\n",
    "    stemmed_vtt_content.append(preprocessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Stopwords are removed from stemmed data to get more relevant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\"\"\"converting stop words to ascii to prevent unicode error \n",
    "and adding some new words that intuitively could be stop words here,afetr looking at the raw data\"\"\"\n",
    "stop_words_ascii=[]\n",
    "for each in stopwords.words('english'):\n",
    "    stop_words_ascii.append(each)\n",
    "    #python2 : stop_words_ascii.append(each.encode('ascii','ignore'))\n",
    "\n",
    "stop_words_ascii.extend(['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z'])\n",
    "stop_words_ascii.extend(['ah','uh','the','and','so','is','ok','um','ok','ha','language:','en'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing stop words from stemmed vtt data\n",
    "\n",
    "filtered_words_list=[]\n",
    "i=0\n",
    "for i in range(len(stemmed_vtt_content)):\n",
    "    filtered_words=[]\n",
    "    #use simple stemmed_vtt_content[i] in python2 as in python2 we have already decoded above\n",
    "    for item in stemmed_vtt_content[i].decode(\"utf-8\").split(\" \"):\n",
    "        if item not in stop_words_ascii:\n",
    "            filtered_words.append(item)         \n",
    "    str1=\" \".join(filtered_words)\n",
    "   \n",
    "    filtered_words_list.append(str1)\n",
    "#print(\"FINAL_____________________________________________________________________\",filtered_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Verifying if words are stemmed and cleaned off the stop words\n",
    "filtered_words_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_frequency_of_relevant_words(filtered_words_list):\n",
    "    import collections\n",
    "    for i in range(len(filtered_words_list)):\n",
    "    #for list_1 in filtered_words_list[i].split(\" \"):\n",
    "        counter = collections.Counter(filtered_words_list[i].split(\" \"))\n",
    "        print(counter.most_common())\n",
    "\n",
    "check_frequency_of_relevant_words(filtered_words_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words : TFIDF Application "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf = TfidfVectorizer(analyzer='word', ngram_range=(1,1), min_df = 0, stop_words = stop_words_ascii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix =  tf.fit_transform(filtered_words_list)\n",
    "feature_names = tf.get_feature_names() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "doc_id=0\n",
    "#showing 50 most important occuring words,as processed by tf-idf\n",
    "tfidf_words_list=[]\n",
    "dense=tfidf_matrix.todense()\n",
    "for i in range(len(filtered_words_list)):\n",
    "    top_n=[]\n",
    "    perVid_Transcript=dense[i].tolist()[0]\n",
    "    phrase_scores= [pair for pair in zip(range(0, len(perVid_Transcript)), perVid_Transcript) if pair[1] > 0]\n",
    "    sorted_phrase_scores=sorted(phrase_scores, key=lambda t: t[1] * -1)\n",
    "    for phrase, score in [(feature_names[word_id], score) for (word_id, score) in sorted_phrase_scores][:50]:\n",
    "        top_n.append(phrase.encode('ascii','ignore').decode(\"utf-8\"))\n",
    "        #for python 2 use: top_n.append(phrase.encode('ascii','ignore')\n",
    "        #print('{0} {1: <50} {2}'.format(i,phrase, score))\n",
    "    tfidf_words_list.append(top_n)\n",
    "    #print('\\n Document {0} :{1}'.format(i,top_n))\n",
    "#print('Final list of 50 words of all docs in one list',tfidf_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding a new column in our dataframe of top bag of words with each vtt\n",
    "nptel_raw_data['TF-IDF_top_words']=tfidf_words_list\n",
    "nptel_raw_data\n",
    "nptel_raw_data.to_csv('nptel_tfidf_labelled.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
