{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_csv('nptel_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "#converting stop words to ascii to prevent unicode error and adding some new words that could intuitively be stop words here\n",
    "vtt_content=df1.vtt_Content.tolist()\n",
    "stop_words_ascii=[]\n",
    "for each in stopwords.words('english'):\n",
    "    stop_words_ascii.append(each.encode('ascii','ignore'))\n",
    "\n",
    "stop_words_ascii.extend(['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z'])\n",
    "stop_words_ascii.extend(['ah','uh','the','and','so','is','ok','um','ok','ha','language:','en'])\n",
    "\n",
    "#print(stop_words_ascii)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#STEMMING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import re\n",
    "\n",
    "porter_stemmer = nltk.stem.porter.PorterStemmer()\n",
    "\n",
    "def porter_tokenizer(text, stemmer=porter_stemmer):\n",
    "    tokens = nltk.wordpunct_tokenize(text)\n",
    "    stems = [porter_stemmer.stem(t) for t in tokens]\n",
    "    no_punct = [s for s in stems if re.match('^[a-zA-Z]+$', s) is not None]\n",
    "    return no_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_vtt_content=[] \n",
    "#stemming each word for each row of vtt_description of videos\n",
    "for eachcontent in vtt_content:\n",
    "    eachcontent=eachcontent.lower().decode('utf-8')\n",
    "    #print(type(eachcontent))\n",
    "    tokenized=porter_tokenizer(eachcontent)\n",
    "    preprocessed=' '.join(tokenized)\n",
    "    preprocessed=preprocessed.encode('ascii','ignore')\n",
    "    #print(preprocessed)\n",
    "    stemmed_vtt_content.append(preprocessed)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing stop words from stemmed vtt data\n",
    "filtered_words_list=[]\n",
    "i=0\n",
    "for i in range(len(stemmed_vtt_content)):\n",
    "    filtered_words=[]\n",
    "    for item in stemmed_vtt_content[i].split(\" \"):\n",
    "        if item not in stop_words_ascii:\n",
    "            filtered_words.append(item)         \n",
    "    #print(filtered_words)\n",
    "    str1=\" \".join(filtered_words)\n",
    "    #print str1\n",
    "    #print(\"\\n NEXT DOCUMENT______________________________________________________________________\",i)\n",
    "    filtered_words_list.append(str1)\n",
    "#print(\"FINAL_____________________________________________________________________\",filtered_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the most often occuring words,utility function only for analysis \n",
    "import collections\n",
    "for i in range(len(filtered_words_list)):\n",
    "    #for list_1 in filtered_words_list[i].split(\" \"):\n",
    "    counter = collections.Counter(filtered_words_list[i].split(\" \"))\n",
    "    #print(counter.most_common())\n",
    "    #print(\"______________________________________________________________________________________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Applying TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf = TfidfVectorizer(analyzer='word', ngram_range=(1,1), min_df = 0, stop_words = stop_words_ascii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix =  tf.fit_transform(filtered_words_list)\n",
    "feature_names = tf.get_feature_names() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14682"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<902x14682 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 392333 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "doc_id=0\n",
    "#showing 50 most important occuring words,as processed by tfidf\n",
    "tfidf_words_list=[]\n",
    "dense=tfidf_matrix.todense()\n",
    "for i in range(len(filtered_words_list)):\n",
    "    top_n=[]\n",
    "    perVid_Transcript=dense[i].tolist()[0]\n",
    "    phrase_scores= [pair for pair in zip(range(0, len(perVid_Transcript)), perVid_Transcript) if pair[1] > 0]\n",
    "    sorted_phrase_scores=sorted(phrase_scores, key=lambda t: t[1] * -1)\n",
    "    for phrase, score in [(feature_names[word_id], score) for (word_id, score) in sorted_phrase_scores][:50]:\n",
    "        top_n.append(phrase.encode('ascii','ignore'))\n",
    "        #print('{0} {1: <50} {2}'.format(i,phrase, score))\n",
    "    tfidf_words_list.append(top_n)\n",
    "    #print('\\n Document {0} :{1}'.format(i,top_n))\n",
    "#print('FINAL list of 50 words of all docs in one list',tfidf_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding a new row in our dataframe of top tags with each vtt\n",
    "df1['TFIDF_top_words']=tfidf_words_list\n",
    "df1\n",
    "df1.to_csv('nptel_tfidf_labelled.csv')\n",
    "\n",
    "#print df1['TFIDF_top_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
